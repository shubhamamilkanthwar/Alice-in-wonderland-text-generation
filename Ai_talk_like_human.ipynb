{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ai- talk like human.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "D5VQprdLpMpG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ef2sb4EQpQBg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load text\n",
        "path_to_file = 'pg11.txt'\n",
        "file = open(path_to_file, 'rt')\n",
        "text = file.read()\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oBpiywp2pRo2",
        "colab_type": "code",
        "outputId": "ac5eff82-f849-473e-90c7-efcb7ee15133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 167516 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s2yg-Xi8pZKe",
        "colab_type": "code",
        "outputId": "13b8f536-5704-461e-b395-324639886492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1300
        }
      },
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:2500])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere at no cost and with\r\n",
            "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
            "re-use it under the terms of the Project Gutenberg License included\r\n",
            "with this eBook or online at www.gutenberg.org\r\n",
            "\r\n",
            "\r\n",
            "Title: Alice's Adventures in Wonderland\r\n",
            "\r\n",
            "Author: Lewis Carroll\r\n",
            "\r\n",
            "Posting Date: June 25, 2008 [EBook #11]\r\n",
            "Release Date: March, 1994\r\n",
            "[Last updated: December 20, 2011]\r\n",
            "\r\n",
            "Language: English\r\n",
            "\r\n",
            "\r\n",
            "*** START OF THIS PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "ALICE'S ADVENTURES IN WONDERLAND\r\n",
            "\r\n",
            "Lewis Carroll\r\n",
            "\r\n",
            "THE MILLENNIUM FULCRUM EDITION 3.0\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "CHAPTER I. Down the Rabbit-Hole\r\n",
            "\r\n",
            "Alice was beginning to get very tired of sitting by her sister on the\r\n",
            "bank, and of having nothing to do: once or twice she had peeped into the\r\n",
            "book her sister was reading, but it had no pictures or conversations in\r\n",
            "it, 'and what is the use of a book,' thought Alice 'without pictures or\r\n",
            "conversations?'\r\n",
            "\r\n",
            "So she was considering in her own mind (as well as she could, for the\r\n",
            "hot day made her feel very sleepy and stupid), whether the pleasure\r\n",
            "of making a daisy-chain would be worth the trouble of getting up and\r\n",
            "picking the daisies, when suddenly a White Rabbit with pink eyes ran\r\n",
            "close by her.\r\n",
            "\r\n",
            "There was nothing so VERY remarkable in that; nor did Alice think it so\r\n",
            "VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\r\n",
            "Oh dear! I shall be late!' (when she thought it over afterwards, it\r\n",
            "occurred to her that she ought to have wondered at this, but at the time\r\n",
            "it all seemed quite natural); but when the Rabbit actually TOOK A WATCH\r\n",
            "OUT OF ITS WAISTCOAT-POCKET, and looked at it, and then hurried on,\r\n",
            "Alice started to her feet, for it flashed across her mind that she had\r\n",
            "never before seen a rabbit with either a waistcoat-pocket, or a watch\r\n",
            "to take out of it, and burning with curiosity, she ran across the field\r\n",
            "after it, and fortunately was just in time to see it pop down a large\r\n",
            "rabbit-hole under the hedge.\r\n",
            "\r\n",
            "In another moment down went Alice after it, never once considering how\r\n",
            "in the world she was to get out again.\r\n",
            "\r\n",
            "The rabbit-hole went straight on like a tunnel for some way, and then\r\n",
            "dipped suddenly down, so suddenly that Alice had not a moment to think\r\n",
            "about stopping herself before she found herself falling down a very deep\r\n",
            "well.\r\n",
            "\r\n",
            "Either the well was very deep, or she fell ver\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X1PHA_ewpbhz",
        "colab_type": "code",
        "outputId": "99f658a8-f751-4340-8dab-244bca03ce68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))\n",
        "vocab1=len(text)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "86 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yiFbEMcB5yJj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DNUnaM-hpe13",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D0YUz3XfpicQ",
        "colab_type": "code",
        "outputId": "c2345132-7f18-44eb-9dca-9b3cfc268df7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "cell_type": "code",
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  '\\r':   1,\n",
            "  ' ' :   2,\n",
            "  '!' :   3,\n",
            "  '\"' :   4,\n",
            "  '#' :   5,\n",
            "  '$' :   6,\n",
            "  '%' :   7,\n",
            "  \"'\" :   8,\n",
            "  '(' :   9,\n",
            "  ')' :  10,\n",
            "  '*' :  11,\n",
            "  ',' :  12,\n",
            "  '-' :  13,\n",
            "  '.' :  14,\n",
            "  '/' :  15,\n",
            "  '0' :  16,\n",
            "  '1' :  17,\n",
            "  '2' :  18,\n",
            "  '3' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "koW0rmy0pkoY",
        "colab_type": "code",
        "outputId": "530802e4-c58a-402c-b8dc-65fc3d02df1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'\\ufeffProject Gute' ---- characters mapped to int ---- > [85 45 76 73 68 63 61 78  2 36 79 78 63]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_jL7l0QopnCn",
        "colab_type": "code",
        "outputId": "e1975abf-7fd1-4818-e7b1-ff560999fee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "cell_type": "code",
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "﻿\n",
            "P\n",
            "r\n",
            "o\n",
            "j\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4ykDWGxOpskZ",
        "colab_type": "code",
        "outputId": "6c1a4337-715a-4d05-9dde-5f40883a3095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"\\ufeffProject Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\\r\\n\\r\\nThis eBook is for the use \"\n",
            "'of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give it '\n",
            "'away or\\r\\nre-use it under the terms of the Project Gutenberg License included\\r\\nwith this eBook or onli'\n",
            "\"ne at www.gutenberg.org\\r\\n\\r\\n\\r\\nTitle: Alice's Adventures in Wonderland\\r\\n\\r\\nAuthor: Lewis Carroll\\r\\n\\r\\nPost\"\n",
            "'ing Date: June 25, 2008 [EBook #11]\\r\\nRelease Date: March, 1994\\r\\n[Last updated: December 20, 2011]\\r\\n\\r\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xDwd1RnIpvYt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mf9ySgZxp0iv",
        "colab_type": "code",
        "outputId": "f995d54f-62be-4c76-e5ad-db5c315c316a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  \"\\ufeffProject Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\\r\\n\\r\\nThis eBook is for the use\"\n",
            "Target data: \"Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\\r\\n\\r\\nThis eBook is for the use \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NHGVe7OMp92H",
        "colab_type": "code",
        "outputId": "8a75b1f7-c32e-4eb9-b78d-4ef3da9f6ce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "cell_type": "code",
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 85 ('\\ufeff')\n",
            "  expected output: 45 ('P')\n",
            "Step    1\n",
            "  input: 45 ('P')\n",
            "  expected output: 76 ('r')\n",
            "Step    2\n",
            "  input: 76 ('r')\n",
            "  expected output: 73 ('o')\n",
            "Step    3\n",
            "  input: 73 ('o')\n",
            "  expected output: 68 ('j')\n",
            "Step    4\n",
            "  input: 68 ('j')\n",
            "  expected output: 63 ('e')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Uevj3Z4xqFBx",
        "colab_type": "code",
        "outputId": "cbfe61df-761f-40fc-8ecb-9cc9b8295f86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 32\n",
        "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: ((32, 100), (32, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "JSnlt1sdqIuK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gkuxHz4rqLRu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if tf.test.is_gpu_available():\n",
        "  rnn = tf.keras.layers.CuDNNGRU\n",
        "else:\n",
        "  import functools\n",
        "  rnn = functools.partial(\n",
        "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oIJN0OXNqO85",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    rnn(rnn_units,\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kvz-1lgrqSP3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sU7s0ZVZqVU0",
        "colab_type": "code",
        "outputId": "377c2a23-174a-4a08-8277-0e1ad32bf15b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 100, 86) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "97L0vJrl2GqI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t8zUvefOqXf7",
        "colab_type": "code",
        "outputId": "9c2f3e17-82d3-4367-d8e8-6845d8410688",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (32, None, 256)           22016     \n",
            "_________________________________________________________________\n",
            "cu_dnngru (CuDNNGRU)         (32, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (32, None, 86)            88150     \n",
            "=================================================================\n",
            "Total params: 4,048,470\n",
            "Trainable params: 4,048,470\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7WGLWGm7qan6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WER28jTlqeoL",
        "colab_type": "code",
        "outputId": "8651a938-997a-4003-de8a-361c4965e000",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([44, 17, 53, 84, 82, 38,  3, 65, 55, 47, 74,  2, 38, 44, 68, 47, 83,\n",
              "        0,  6, 73, 17, 12, 76, 30, 55, 68, 24, 45, 73, 60, 82, 34, 77, 52,\n",
              "       29, 55, 56, 10, 67, 65, 68, 41, 23, 77, 39, 66, 37, 52, 61, 80, 30,\n",
              "       65,  3,  5,  2, 63, 50, 70, 15,  8, 75, 20, 22, 11, 35, 23, 46, 25,\n",
              "       71, 12, 34,  1, 46, 53, 39, 53, 53, 70, 23, 60, 41, 12, 77, 25, 54,\n",
              "       21, 73, 77, 31, 56, 65,  2, 58, 47, 33,  7, 61, 72, 34, 55])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "lMDq7o_Cqh0r",
        "colab_type": "code",
        "outputId": "263daca9-0bd1-4ffc-9844-562e9b2519f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[15]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " ' executioner, the King, and the Queen, who were all talking at once,\\r\\nwhile all the rest were quite '\n",
            "\n",
            "Next Char Predictions: \n",
            " \"O1XzxI!gZRp IOjRy\\n$o1,rAZj8PobxEsW@Z[)igjL7sJhHWcvAg!# eUl/'q46*F7Q9m,E\\rQXJXXl7bL,s9Y5osB[g _RD%cnEZ\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B7FHe7J3qtB6",
        "colab_type": "code",
        "outputId": "09eecaa7-6ea1-4556-ac01-99b05ea15d87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (32, 100, 86)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.452918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aRQlGR8Hqza9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer = tf.train.AdamOptimizer(),\n",
        "    loss = loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ndCoSiAE2izm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XFpNVqPNq9ms",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P3q7jsfuq1ob",
        "colab_type": "code",
        "outputId": "acff375e-5580-4093-e36e-ad4b4bed9935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1823
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset.repeat(), epochs=50, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "51/52 [============================>.] - ETA: 0s - loss: 3.5841WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
            "52/52 [==============================] - 6s 120ms/step - loss: 3.5678\n",
            "Epoch 2/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 2.4863\n",
            "Epoch 3/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 2.2445\n",
            "Epoch 4/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 2.0607\n",
            "Epoch 5/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 1.9104\n",
            "Epoch 6/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 1.7726\n",
            "Epoch 7/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 1.6646\n",
            "Epoch 8/50\n",
            "52/52 [==============================] - 5s 98ms/step - loss: 1.5522\n",
            "Epoch 9/50\n",
            "52/52 [==============================] - 5s 98ms/step - loss: 1.4686\n",
            "Epoch 10/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 1.3785\n",
            "Epoch 11/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 1.3096\n",
            "Epoch 12/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 1.2359\n",
            "Epoch 13/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 1.1686\n",
            "Epoch 14/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 1.1077\n",
            "Epoch 15/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 1.0485\n",
            "Epoch 16/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.9844\n",
            "Epoch 17/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.9222\n",
            "Epoch 18/50\n",
            "52/52 [==============================] - 5s 100ms/step - loss: 0.8620\n",
            "Epoch 19/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.7992\n",
            "Epoch 20/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.7360\n",
            "Epoch 21/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.6815\n",
            "Epoch 22/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.6223\n",
            "Epoch 23/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.5692\n",
            "Epoch 24/50\n",
            "52/52 [==============================] - 5s 98ms/step - loss: 0.5199\n",
            "Epoch 25/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.4703\n",
            "Epoch 26/50\n",
            "52/52 [==============================] - 5s 98ms/step - loss: 0.4348\n",
            "Epoch 27/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.3989\n",
            "Epoch 28/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.3696\n",
            "Epoch 29/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.3450\n",
            "Epoch 30/50\n",
            "52/52 [==============================] - 5s 98ms/step - loss: 0.3215\n",
            "Epoch 31/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.3073\n",
            "Epoch 32/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.2884\n",
            "Epoch 33/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.2779\n",
            "Epoch 34/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.2653\n",
            "Epoch 35/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.2552\n",
            "Epoch 36/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.2505\n",
            "Epoch 37/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.2408\n",
            "Epoch 38/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.2362\n",
            "Epoch 39/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.2274\n",
            "Epoch 40/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.2226\n",
            "Epoch 41/50\n",
            "52/52 [==============================] - 5s 98ms/step - loss: 0.2189\n",
            "Epoch 42/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.2154\n",
            "Epoch 43/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.2127\n",
            "Epoch 44/50\n",
            "52/52 [==============================] - 5s 98ms/step - loss: 0.2051\n",
            "Epoch 45/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.2034\n",
            "Epoch 46/50\n",
            "52/52 [==============================] - 5s 98ms/step - loss: 0.2013\n",
            "Epoch 47/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.2031\n",
            "Epoch 48/50\n",
            "52/52 [==============================] - 5s 99ms/step - loss: 0.1976\n",
            "Epoch 49/50\n",
            "52/52 [==============================] - 5s 98ms/step - loss: 0.1952\n",
            "Epoch 50/50\n",
            "52/52 [==============================] - 5s 98ms/step - loss: 0.1947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Im3qz7C9q5F-",
        "colab_type": "code",
        "outputId": "273a434d-9236-4998-b1b8-51acc23ee43d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_50'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "Dc0JI4gtrBWg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wJ_JxPiUrFlw",
        "colab_type": "code",
        "outputId": "09beb2b4-1111-4d98-e919-7fa43086b3b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            22016     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_1 (CuDNNGRU)       (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 86)             88150     \n",
            "=================================================================\n",
            "Total params: 4,048,470\n",
            "Trainable params: 4,048,470\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MeDmuSSQrZIH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 500\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a multinomial distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "azHJQ3XArbqg",
        "colab_type": "code",
        "outputId": "975260a1-0846-4c17-dac7-8ef960c4f316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=u\"sister   : \"))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sister   : then and thener.\r\n",
            "\r\n",
            "'It isn't a little bird and got no business there.\r\n",
            "\r\n",
            "'Did they livenked to have forgotten the words.'\r\n",
            "\r\n",
            "So they sat down the court, she ran as mat.'\r\n",
            "\r\n",
            "This was said that day.\r\n",
            "\r\n",
            "'Then it off that in some bookenberg-tm electronic\r\n",
            "works,\r\n",
            "and began tm eBooks, remove the firelt stapping on the\r\n",
            "Queen: the only one who got any a piece of bread-and-butter in the other. In the verses on his knee, and looking at them\r\n",
            "with one eye; 'I seem the Mock Turtle: 'why looked at the\r\n",
            "Qu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f7ZfE7wRreTS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}